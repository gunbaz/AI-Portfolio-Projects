# ğŸ–Šï¸ Handwritten Digit Recognition (MNIST)

[![Python](https://img.shields.io/badge/Python-3.11-blue.svg)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.5.1-orange.svg)](https://pytorch.org/)
[![CUDA](https://img.shields.io/badge/CUDA-12.4-green.svg)](https://developer.nvidia.com/cuda-toolkit)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

A deep learning project for recognizing handwritten digits (0-9) from the MNIST dataset using PyTorch. This project implements a custom Convolutional Neural Network (CNN) architecture inspired by LeNet-5, achieving state-of-the-art performance with **99.47% test accuracy**.

## ğŸ¯ Project Overview

This project demonstrates a complete deep learning pipeline for image classification, from data exploration to model deployment. The model successfully classifies handwritten digits with high accuracy using a modern CNN architecture with BatchNorm and Dropout regularization techniques.

**Key Highlights:**
- âœ¨ **99.47% test accuracy** on MNIST test set
- ğŸ—ï¸ Custom CNN architecture with 871,530 trainable parameters
- ğŸ“Š Comprehensive EDA (Exploratory Data Analysis) with visualizations
- ğŸ”§ Well-structured, modular codebase following best practices
- ğŸ“ˆ Training history tracking with early stopping
- ğŸ¨ Professional visualizations and evaluation metrics

## ğŸ“Š Results

### Performance Metrics

| Metric | Value |
|--------|-------|
| **Test Accuracy** | **99.47%** |
| Validation Accuracy | 99.43% |
| Best Epoch | 13 |
| Total Parameters | 871,530 |
| Training Time | ~15 epochs |

**Note:** All classes achieved F1-score â‰¥ 0.99, demonstrating excellent performance across all digit classes (0-9).

### Classification Report Summary

- **Precision:** 0.99 (macro avg)
- **Recall:** 0.99 (macro avg)
- **F1-Score:** 0.99 (macro avg)
- **Support:** 10,000 test samples

## ğŸ–¼ï¸ Visualizations

### Sample Images
![Sample Images](outputs/figures/sample_images.png)
*25 randomly selected samples from the MNIST training dataset*

### Training History
![Training History](outputs/figures/training_history.png)
*Training and validation loss/accuracy curves showing model convergence*

### Confusion Matrix
![Confusion Matrix](outputs/figures/confusion_matrix.png)
*Confusion matrix showing classification performance across all 10 digit classes*

### Per-Class Accuracy
![Per-Class Accuracy](outputs/figures/per_class_accuracy.png)
*Accuracy breakdown for each digit class (0-9)*

### Misclassified Examples
![Misclassified Examples](outputs/figures/misclassified_examples.png)
*Visualization of incorrectly classified samples for error analysis*

## ğŸ—ï¸ Model Architecture

The model uses a custom CNN architecture inspired by LeNet-5 with modern improvements:

### Architecture Overview

| Layer | Type | Output Shape | Parameters |
|-------|------|--------------|------------|
| **Conv Block 1** | Conv2d â†’ BatchNorm â†’ ReLU | (32, 28, 28) | 9,632 |
| | Conv2d â†’ BatchNorm â†’ ReLU | (32, 28, 28) | 9,248 |
| | MaxPool2d â†’ Dropout2d | (32, 14, 14) | - |
| **Conv Block 2** | Conv2d â†’ BatchNorm â†’ ReLU | (64, 14, 14) | 18,496 |
| | Conv2d â†’ BatchNorm â†’ ReLU | (64, 14, 14) | 36,928 |
| | MaxPool2d â†’ Dropout2d | (64, 7, 7) | - |
| **Fully Connected** | Flatten â†’ Linear | (256) | 803,072 |
| | BatchNorm1d â†’ ReLU â†’ Dropout | (256) | 512 |
| | Linear | (10) | 2,570 |

**Total Parameters:** 871,530

### Key Features
- **Batch Normalization:** Stabilizes training and improves convergence
- **Dropout Regularization:** Prevents overfitting (0.25 for conv layers, 0.5 for FC)
- **Modern Architecture:** LeNet-5 inspired with contemporary improvements

## ğŸ“ Project Structure

```
Handwritten-Digit-Recognition/
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ config.yaml          # Configuration file
â”œâ”€â”€ data/
â”‚   â””â”€â”€ MNIST/               # MNIST dataset (auto-downloaded)
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ exploration.ipynb    # EDA notebook
â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ figures/             # Visualization outputs
â”‚   â””â”€â”€ models/               # Trained model checkpoints
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_loader.py        # Data loading utilities
â”‚   â”œâ”€â”€ model.py              # CNN model architecture
â”‚   â”œâ”€â”€ train.py              # Training script
â”‚   â”œâ”€â”€ evaluate.py           # Evaluation script
â”‚   â””â”€â”€ utils.py              # Utility functions
â”œâ”€â”€ requirements.txt          # Python dependencies
â””â”€â”€ README.md                 # This file
```

## ğŸš€ Quick Start

### Prerequisites

- Python 3.11.0
- CUDA 12.4+ (for GPU acceleration)
- NVIDIA GPU with CUDA support (recommended)

### Installation

1. **Clone the repository**
   ```bash
   git clone <repository-url>
   cd Computer-Vision/Handwritten-Digit-Recognition
   ```

2. **Create virtual environment**
   ```bash
   python -m venv venv
   # Windows
   venv\Scripts\activate
   # Linux/Mac
   source venv/bin/activate
   ```

3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

### Usage

**Training:**
```bash
python src/train.py
```

**Evaluation:**
```bash
python src/evaluate.py
```

**EDA Notebook:**
```bash
jupyter notebook notebooks/exploration.ipynb
```

## ğŸ› ï¸ Technologies Used

- ![PyTorch](https://img.shields.io/badge/PyTorch-2.5.1-orange) - Deep learning framework
- ![NumPy](https://img.shields.io/badge/NumPy-1.26.4-blue) - Numerical computing
- ![Matplotlib](https://img.shields.io/badge/Matplotlib-3.8.2-green) - Data visualization
- ![Seaborn](https://img.shields.io/badge/Seaborn-0.13.2-purple) - Statistical visualization
- ![scikit-learn](https://img.shields.io/badge/scikit--learn-1.4.0-yellow) - Machine learning utilities
- ![Jupyter](https://img.shields.io/badge/Jupyter-1.0.0-orange) - Interactive notebooks

## ğŸ“ˆ Training Details

### Hyperparameters

| Parameter | Value |
|-----------|-------|
| Epochs | 15 |
| Batch Size | 64 |
| Learning Rate | 0.001 |
| Optimizer | Adam |
| Weight Decay | 0.0001 (L2 regularization) |
| Loss Function | CrossEntropyLoss |
| Early Stopping Patience | 5 epochs |

### Training Configuration

- **Data Split:** 90% train, 10% validation
- **Data Augmentation:** None (baseline model)
- **Normalization:** Mean=0.1307, Std=0.3081 (MNIST standard)
- **Device:** CUDA (GPU acceleration)

### Training Process

The model was trained with:
- Early stopping to prevent overfitting
- Best model checkpoint saving based on validation loss
- Progress tracking with tqdm
- Comprehensive logging and visualization

## ğŸ“ Key Learnings

1. **Architecture Design:** Modern CNN architectures with BatchNorm and Dropout can achieve excellent performance even on relatively simple datasets like MNIST.

2. **Data Exploration:** Comprehensive EDA helps understand data distribution, identify potential issues, and guide model design decisions.

3. **Regularization:** Proper use of BatchNorm and Dropout significantly improves generalization and prevents overfitting.

4. **Evaluation:** Detailed evaluation metrics (confusion matrix, per-class accuracy) provide insights beyond overall accuracy, helping identify model weaknesses.

5. **Code Organization:** Modular code structure with separate modules for data loading, model definition, training, and evaluation improves maintainability and reusability.

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

---

**Note:** This project is part of an AI Portfolio showcasing various machine learning and deep learning projects. For more projects, visit the main repository.

