# Evaluation metrics

import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
from pathlib import Path
from typing import Dict, Tuple
import os


def load_model(config: Dict, model_path: str, device: torch.device) -> nn.Module:
    """
    Modeli checkpoint'tan yÃ¼kler.
    
    Args:
        config: KonfigÃ¼rasyon dictionary'si
        model_path: Model checkpoint dosya yolu
        device: Cihaz (cuda/cpu)
    
    Returns:
        nn.Module: YÃ¼klenmiÅŸ ve eval moduna alÄ±nmÄ±ÅŸ model
    """
    from src.model import MNISTNet
    from src.train import load_checkpoint
    
    # Model oluÅŸtur
    model = MNISTNet(config).to(device)
    
    # Checkpoint yÃ¼kle
    checkpoint = load_checkpoint(model_path, model)
    
    # Eval moduna al
    model.eval()
    
    return model


def get_predictions(
    model: nn.Module,
    test_loader: torch.utils.data.DataLoader,
    device: torch.device
) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    """
    TÃ¼m test seti Ã¼zerinde tahmin yapar.
    
    Args:
        model: DeÄŸerlendirilecek model
        test_loader: Test DataLoader
        device: Cihaz (cuda/cpu)
    
    Returns:
        Tuple[np.ndarray, np.ndarray, np.ndarray]: (predictions, labels, probabilities)
    """
    all_predictions = []
    all_labels = []
    all_probabilities = []
    
    model.eval()
    with torch.no_grad():
        for images, labels in test_loader:
            images = images.to(device)
            labels = labels.to(device)
            
            # Forward pass
            outputs = model(images)
            
            # Softmax ile probability hesapla
            probabilities = torch.softmax(outputs, dim=1)
            
            # Tahminleri al
            _, predicted = torch.max(outputs, 1)
            
            # CPU'ya taÅŸÄ± ve numpy'ye Ã§evir
            all_predictions.extend(predicted.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
            all_probabilities.extend(probabilities.cpu().numpy())
    
    return np.array(all_predictions), np.array(all_labels), np.array(all_probabilities)


def plot_confusion_matrix(
    y_true: np.ndarray,
    y_pred: np.ndarray,
    class_names: list,
    save_path: str
) -> None:
    """
    Confusion matrix'i gÃ¶rselleÅŸtirir ve kaydeder.
    
    Args:
        y_true: GerÃ§ek etiketler
        y_pred: Tahmin edilen etiketler
        class_names: SÄ±nÄ±f isimleri listesi
        save_path: KayÄ±t yolu
    """
    # Confusion matrix hesapla
    cm = confusion_matrix(y_true, y_pred)
    
    # GÃ¶rselleÅŸtir
    plt.figure(figsize=(10, 8))
    sns.heatmap(
        cm,
        annot=True,
        fmt='d',
        cmap='Blues',
        xticklabels=class_names,
        yticklabels=class_names,
        cbar_kws={'label': 'Ã–rnek SayÄ±sÄ±'}
    )
    plt.title('Confusion Matrix - MNIST Test Seti', fontsize=16, fontweight='bold', pad=20)
    plt.xlabel('Tahmin Edilen', fontsize=12)
    plt.ylabel('GerÃ§ek', fontsize=12)
    plt.tight_layout()
    
    # Kaydet
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    print(f"âœ… Confusion matrix kaydedildi: {save_path}")
    plt.close()


def plot_misclassified_examples(
    images: np.ndarray,
    true_labels: np.ndarray,
    pred_labels: np.ndarray,
    class_names: list,
    save_path: str,
    num_examples: int = 25
) -> None:
    """
    YanlÄ±ÅŸ tahmin edilen Ã¶rnekleri gÃ¶rselleÅŸtirir.
    
    Args:
        images: GÃ¶rÃ¼ntÃ¼ array'i (normalize edilmiÅŸ)
        true_labels: GerÃ§ek etiketler
        pred_labels: Tahmin edilen etiketler
        class_names: SÄ±nÄ±f isimleri listesi
        save_path: KayÄ±t yolu
        num_examples: GÃ¶sterilecek Ã¶rnek sayÄ±sÄ± (varsayÄ±lan: 25)
    """
    # YanlÄ±ÅŸ tahmin edilenleri bul
    misclassified_indices = np.where(true_labels != pred_labels)[0]
    
    if len(misclassified_indices) == 0:
        print("âš ï¸  YanlÄ±ÅŸ tahmin edilen Ã¶rnek bulunamadÄ±!")
        return
    
    # Ä°lk num_examples kadarÄ±nÄ± al
    num_examples = min(num_examples, len(misclassified_indices))
    selected_indices = misclassified_indices[:num_examples]
    
    # Denormalize et (gÃ¶rselleÅŸtirme iÃ§in)
    mean = 0.1307
    std = 0.3081
    images_denorm = images * std + mean
    images_denorm = np.clip(images_denorm, 0, 1)
    
    # Grid oluÅŸtur
    rows = int(np.ceil(np.sqrt(num_examples)))
    cols = int(np.ceil(num_examples / rows))
    
    fig, axes = plt.subplots(rows, cols, figsize=(cols * 2, rows * 2))
    fig.suptitle('YanlÄ±ÅŸ Tahmin Edilen Ã–rnekler', fontsize=16, fontweight='bold')
    
    # Eksenleri dÃ¼zleÅŸtir
    if rows == 1:
        axes = axes.reshape(1, -1)
    elif cols == 1:
        axes = axes.reshape(-1, 1)
    else:
        axes = axes.flatten()
    
    for idx, mis_idx in enumerate(selected_indices):
        ax = axes[idx] if num_examples > 1 else axes
        
        # GÃ¶rÃ¼ntÃ¼yÃ¼ gÃ¶ster
        img = images_denorm[mis_idx].squeeze()
        ax.imshow(img, cmap='gray')
        
        # BaÅŸlÄ±k (kÄ±rmÄ±zÄ± renkte)
        true_label = class_names[true_labels[mis_idx]]
        pred_label = class_names[pred_labels[mis_idx]]
        ax.set_title(f'GerÃ§ek: {true_label}, Tahmin: {pred_label}', 
                    color='red', fontsize=10, fontweight='bold')
        ax.axis('off')
    
    # KullanÄ±lmayan eksenleri gizle
    for idx in range(num_examples, len(axes)):
        axes[idx].axis('off')
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    print(f"âœ… YanlÄ±ÅŸ tahmin edilen Ã¶rnekler kaydedildi: {save_path}")
    plt.close()


def plot_per_class_accuracy(
    y_true: np.ndarray,
    y_pred: np.ndarray,
    class_names: list,
    save_path: str
) -> None:
    """
    Her sÄ±nÄ±f iÃ§in accuracy'yi gÃ¶rselleÅŸtirir.
    
    Args:
        y_true: GerÃ§ek etiketler
        y_pred: Tahmin edilen etiketler
        class_names: SÄ±nÄ±f isimleri listesi
        save_path: KayÄ±t yolu
    """
    # Her sÄ±nÄ±f iÃ§in accuracy hesapla
    per_class_acc = []
    for i in range(len(class_names)):
        mask = y_true == i
        if mask.sum() > 0:
            acc = (y_pred[mask] == i).sum() / mask.sum() * 100
        else:
            acc = 0.0
        per_class_acc.append(acc)
    
    # Horizontal bar chart
    fig, ax = plt.subplots(figsize=(10, 6))
    bars = ax.barh(class_names, per_class_acc, color='steelblue', alpha=0.7)
    
    # DeÄŸerleri bar Ã¼zerinde gÃ¶ster
    for i, (bar, acc) in enumerate(zip(bars, per_class_acc)):
        ax.text(acc + 0.5, i, f'{acc:.2f}%', 
               va='center', fontsize=10, fontweight='bold')
    
    ax.set_xlabel('Accuracy (%)', fontsize=12)
    ax.set_ylabel('SÄ±nÄ±f', fontsize=12)
    ax.set_title('SÄ±nÄ±f BazlÄ± Accuracy - MNIST Test Seti', 
                fontsize=14, fontweight='bold')
    ax.set_xlim([0, 105])
    ax.grid(axis='x', alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    print(f"âœ… SÄ±nÄ±f bazlÄ± accuracy grafiÄŸi kaydedildi: {save_path}")
    plt.close()


def evaluate_model(config: Dict) -> Dict:
    """
    Ana deÄŸerlendirme fonksiyonu.
    
    Args:
        config: KonfigÃ¼rasyon dictionary'si
    
    Returns:
        Dict: DeÄŸerlendirme sonuÃ§larÄ±
    """
    from src.utils import get_device
    from src.data_loader import get_data_loaders, get_class_names
    
    # CihazÄ± al
    device = get_device()
    
    # Test loader'Ä± oluÅŸtur
    print("\nðŸ“¦ Test veri seti yÃ¼kleniyor...")
    train_loader, val_loader, test_loader = get_data_loaders(config)
    
    # Best model yolunu al
    model_save_dir = config["paths"]["model_save_dir"]
    best_model_name = config["paths"]["best_model_name"]
    best_model_path = os.path.join(model_save_dir, best_model_name)
    
    # Model yÃ¼kle
    print(f"\nðŸ“¥ Model yÃ¼kleniyor: {best_model_path}")
    if not os.path.exists(best_model_path):
        raise FileNotFoundError(f"Model dosyasÄ± bulunamadÄ±: {best_model_path}")
    
    model = load_model(config, best_model_path, device)
    
    # Tahminleri al
    print("\nðŸ”® Test seti Ã¼zerinde tahmin yapÄ±lÄ±yor...")
    predictions, labels, probabilities = get_predictions(model, test_loader, device)
    
    # Metrikleri hesapla
    accuracy = accuracy_score(labels, predictions) * 100
    cm = confusion_matrix(labels, predictions)
    class_names = get_class_names()
    report = classification_report(labels, predictions, target_names=class_names)
    
    # SonuÃ§larÄ± yazdÄ±r
    print("\n" + "=" * 60)
    print("DEÄžERLENDÄ°RME SONUÃ‡LARI")
    print("=" * 60)
    print(f"\nðŸ“Š Genel Accuracy: {accuracy:.2f}%")
    print(f"\nðŸ“‹ Classification Report:")
    print(report)
    print(f"\nðŸ“ˆ Confusion Matrix:")
    print(cm)
    
    # Grafikleri oluÅŸtur ve kaydet
    figures_dir = config["paths"]["figures_dir"]
    os.makedirs(figures_dir, exist_ok=True)
    
    print("\nðŸ“Š Grafikler oluÅŸturuluyor...")
    
    # Confusion matrix
    cm_path = os.path.join(figures_dir, "confusion_matrix.png")
    plot_confusion_matrix(labels, predictions, class_names, cm_path)
    
    # YanlÄ±ÅŸ tahmin edilen Ã¶rnekler iÃ§in gÃ¶rÃ¼ntÃ¼leri al
    # Test loader'dan gÃ¶rÃ¼ntÃ¼leri topla
    all_images = []
    all_labels_list = []
    for images, labels_batch in test_loader:
        all_images.append(images.numpy())
        all_labels_list.append(labels_batch.numpy())
    all_images = np.concatenate(all_images, axis=0)
    all_labels_array = np.concatenate(all_labels_list, axis=0)
    
    # Misclassified examples
    misclassified_path = os.path.join(figures_dir, "misclassified_examples.png")
    plot_misclassified_examples(
        all_images, labels, predictions, class_names, misclassified_path
    )
    
    # Per-class accuracy
    per_class_path = os.path.join(figures_dir, "per_class_accuracy.png")
    plot_per_class_accuracy(labels, predictions, class_names, per_class_path)
    
    # SonuÃ§larÄ± dÃ¶ndÃ¼r
    results = {
        'accuracy': accuracy,
        'classification_report': report,
        'confusion_matrix': cm
    }
    
    return results


if __name__ == "__main__":
    import sys
    sys.path.append(".")
    from src.utils import load_config, set_seed, print_system_info
    
    print_system_info()
    config = load_config()
    set_seed(config["seed"])
    
    print("\n" + "=" * 50)
    print("MODEL DEÄžERLENDÄ°RMESÄ° BAÅžLIYOR")
    print("=" * 50)
    
    results = evaluate_model(config)
    
    print("\n" + "=" * 50)
    print("DEÄžERLENDÄ°RME TAMAMLANDI")
    print(f"Test Accuracy: {results['accuracy']:.2f}%")
    print("=" * 50)
